{
 "metadata": {
  "name": "",
  "signature": "sha256:a27dd7cf1b50eab5c206705810eec4d71a92c8c00c3eb239cc6c722376efc01b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Generate data on Github team"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import urllib2\n",
      "import seaborn\n",
      "import ConfigParser\n",
      "import github as gh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#set up API access\n",
      "#config file assumed to be in your home directory \n",
      "config = ConfigParser.ConfigParser()\n",
      "config.read('/home/mackenza/.history_git/settings.conf')\n",
      "\n",
      "user = config.get('github', 'user')\n",
      "password = config.get('github', 'password')\n",
      "github = gh.Github(user, password)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the Github team page and scrape all urls\n",
      "url = 'https://github.com/about/team'\n",
      "resp = urllib2.urlopen(url)\n",
      "soup = BeautifulSoup(resp.read(), from_encoding=resp.info().getparam('charset'))\n",
      "links = []\n",
      "for link in soup.find_all('a', href=True):\n",
      "        links.append(link['href'])\n",
      "\n",
      "# choose the URLS that are actually team names\n",
      "team = links[12:]\n",
      "team = team[:233]\n",
      "team.append('/mojombo')\n",
      "team = [t.replace('/', '') for t in team]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Retrieve data on Github team from Github API"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use Github API to get data on these users -- only 230 of them, so not too hard\n",
      "pre = 'https://github.com'\n",
      "team_urls = [pre+t for t in team]\n",
      "# this will take a few minutes as it relies on the GitHub API\n",
      "team_df = pd.DataFrame([github.get_user(t).raw_data for t in team])\n",
      "team_df.to_csv('data/github_team.csv', encoding='utf-8', sep='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Retrieve repos used by the Github team"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# retrieve repo event activity for members of the team\n",
      "def retrieve_repo_event_summary_from_BigQuery(team_list):\n",
      "    # to retrieve all repositories and events for each team member\n",
      "    # use Google BigQuery githubarchive timeline\n",
      "\n",
      "    query = \"\"\"SELECT actor, repository_url, type, count(type) as event_count \n",
      "                    FROM [githubarchive:github.timeline]\n",
      "                     where  actor = '{}' group each by actor, repository_url, type order by event_count desc LIMIT 1000\"\"\"\n",
      "    all_team_repos = pd.DataFrame()\n",
      "    for t in team_list:\n",
      "        team_query = query.format(t)\n",
      "        print team_query\n",
      "        df = pd.io.gbq.read_gbq(team_query)\n",
      "        print df.shape\n",
      "        all_team_repos = pd.concat([all_team_repos, df])\n",
      "    print 'retrieved {} repositories for {} github users'.format(all_team_repos.shape[0], len(team_list))\n",
      "    return all_team_repos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# running this line will take quite a while as it is querying BigQuery several hundred times\n",
      "all_team_repos = retrieve_repo_event_summary_from_BigQuery(team_df.login.values)\n",
      "all_team_repos.to_csv('data/github_team_repos.csv', sep='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Generate summary of events sorted by actor\n",
      "\n",
      "This list of top 10,000 actors is just to help sort out where the Github team fits in the overall activity on Github. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evt_query = \"\"\"SELECT actor, count(type) as event_count FROM [githubarchive:github.timeline] \n",
      "group each by actor\n",
      "order by event_count desc\n",
      "LIMIT 100000\"\"\"\n",
      "\n",
      "evt_df = pd.io.gbq.read_gbq(evt_query)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Waiting on bqjob_r3b34e8ec9687861d_00000146ab341d85_2 ... (1s) Current status: RUNNING "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Waiting on bqjob_r3b34e8ec9687861d_00000146ab341d85_2 ... (4s) Current status: RUNNING "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Waiting on bqjob_r3b34e8ec9687861d_00000146ab341d85_2 ... (7s) Current status: RUNNING "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Waiting on bqjob_r3b34e8ec9687861d_00000146ab341d85_2 ... (8s) Current status: RUNNING "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Waiting on bqjob_r3b34e8ec9687861d_00000146ab341d85_2 ... (10s) Current status: RUNNING "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Waiting on bqjob_r3b34e8ec9687861d_00000146ab341d85_2 ... (13s) Current status: RUNNING "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Waiting on bqjob_r3b34e8ec9687861d_00000146ab341d85_2 ... (13s) Current status: DONE   \n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evt_df.to_csv('data/event_counts_top100k.csv', sep='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}